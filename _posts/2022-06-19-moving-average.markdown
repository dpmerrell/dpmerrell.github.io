---
layout: post
title:  "Are Moving Averages Bayesian?"
date:   2022-06-19 00:00:00 -0500
category: technical 
tags: [bayesian, bayesian-statistics, probability, time-series] 
---

[Moving averages](https://en.wikipedia.org/wiki/Moving_average) are among the most commonly-used tools for "de-noising" time series data.

People usually describe a moving average as a convolutional filter.
This opens the door to many mathematical insights.
For example, [the convolution theorem](https://en.wikipedia.org/wiki/Convolution_theorem) connects convolution in time with point-wise multiplication in Fourier space.

However, I've always suspected moving averages have a Bayesian interpretation. 
The [Wikipedia page](https://en.wikipedia.org/wiki/Moving_average) doesn't mention any, though.
I messed around on a whiteboard and arrived at some interesting insights:

* The "correct" weights for a moving average (given a particular model of the system)
* The Bayesian justification for moving averages is weaker than I would have thought.


# De-noising as Bayesian inference

Suppose our time series are evenly spaced, and generated by a hidden Markov model (HMM).

That is, the observed data \\(x_1, x_2, \ldots \\) are _noisy observations_ of a hidden state that evolves over time, \\(h_1, h_2, \ldots \\).

The following image depicts this situation with a Bayesian network:

![hidden markov model]({{ site.baseurl }}/assets/images/hmm.svg){:width="500px"}


For our purposes, let's assume Gaussian conditional probabilities:

$$ h_i ~|~ h_{i-1} \sim \mathcal{N}(h_{i-1}, \sigma^2) $$

$$ x_i ~|~ h_i \sim \mathcal{N}(h_i, \xi^2) $$

This is consistent with a (discrete) Wiener process (with uncertainty in the observations).
You could reasonably model e.g., log-stock prices this way.

We can frame the "de-noising" task as a Bayesian posterior inference.
Specifically, we want to use the _observed data_ to gain knowledge about each _hidden state_ \\(h_i\\).
This consists of the marginal posterior for \\(h_i\\):

$$P(h_i ~|~ \ldots, x_{i-1}, x_i, x_{i+1}, \ldots) \propto P(\ldots, x_{i-1}, x_i, x_{i+1},\ldots ~|~ h_i) \cdot P(h_i)$$

For the remainder of this post, we will focus on finding a maximum likelihood estimate (MLE) for \\(h_i\\).
That is, we discard the prior \\(P(h_i)\\) and find the maximum of \\(P(\ldots, x_{i-1}, x_i, x_{i+1},\ldots ~|~ h_i)\\).

# Connecting HMMs to moving averages

Given our probabilistic assumptions, we end up with the following log-likelihood for \\(h_i\\):

$$ \mathcal{L}(h_i) = \sum_j \frac{(h_i - x_j)^2}{2(|i-j|\cdot \sigma^2 + \xi^2)} $$

Differentiating with respect to \\(h_i\\) yields the following optimality condition:

$$ \frac{\partial \mathcal{L}}{\partial h_i} = \sum_j \frac{(h_i - x_j)}{|i-j|\cdot \sigma^2 + \xi^2}  = 0$$

Now define 

$$ w_j = \frac{1}{|i-j|\cdot\sigma^2 + \xi^2}. $$

Rearranging the terms of the optimality condition yields the following MLE:

$$ h_i^\ast = \frac{\sum_j w_j x_j }{ \sum_j w_j} $$

Which is exactly a moving average for \\(h_i\\).

This moving average has some puzzling (though interesting) properties:

* The moving average uses an unusual "window" proportional to the inverse of \\(|i-j|\\).
  I haven't seen such a window discussed in popular references on moving averages.
* If we had infinite data, the sums used to compute \\(h_i^\ast\\) would not converge.
  This is surprising and unsatisfying.
  This probably isn't an issue in practical settings&mdash;the sums grow logarithmically and we always have finite data.
* I haven't figured out a way to maximize likelihood for the parameters \\(\sigma^2\\) or \\(\xi^2\\) yet.


# Last remarks

It's interesting to find a connection between HMMs and moving averages.
However, it's a surprisingly weak connection.

I'd go so far as to claim _moving averages aren't really Bayesian_.

If somebody chooses to model time series data with a HMM, then they ought to consider more sophisticated tools than moving averages:

* The [forward-backward algorithm](https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm) can efficiently maximize posterior marginals.
* The [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) can compute the single most likely _hidden history_&mdash;the entire sequence of hidden states \\(h_1, \h_2, \ldots\\).
* In settings with linear dynamics a [Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter) model may also be appropriate. 


\\( \blacksquare\\)  

