---
layout: post
title: "Hair-splitting: machine learning vs. statistics"
date:   2019-11-05 00:00:00 -0500
category: personal
tags: [machine learning, statistics, ontology, opinion] 
---

I say "hair-splitting," but I think the philosophical difference between these fields is actually pretty profound.

<br>

* **Statistics**
    - Attempts to answer questions about _truth_; epistemology. Rather idealistic.
    - Proposes a set of possible models, and posits the existence of a "true" model within that set. This "true" model explains the observations.
        * This description applies to frequentist _and_ bayesian statistics.
        * Both camps take their models seriously as descriptions of reality. They only differ in their _description of epistemic state_, and in their _methods_ for generating that description.
    - Attempts to quantify our epistemic state.
        * Frequentists: confidence intervals, \\(p\\)-values
        * Bayesians: posterior distributions; Bayes factors

<br> 

* **Machine Learning**
    - More pragmatic: what is _useful_?
    - Seeks a model for accomplishing some task: e.g., prediction or decision making.
    - Selects a model based on its performance at that task&mdash;_fitness for purpose_. (A splash of Darwinism.)
    - Makes no pretense at determining truth&mdash;is fundamentally an _engineering_ discipline.
        * We may choose different models based on trade-offs between performance, available memory, available processing power, available data, or other details of the operating environment.
    - Machine learning takes _algorithmic_ concerns into account.
        * Is there an efficient procedure for determining the best model?
        * This is the result of its origins within computer science.
    
<br>

My personal inclinations can probably be guessed from these descriptions.

\\( \blacksquare\\)  

